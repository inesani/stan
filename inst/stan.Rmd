---
title: "The Statistical Android"
author: "Inés Añíbarro Gorostiza"
date: "October 2018"
output: pdf_document
params:
  test: 'default'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, warning=FALSE, results='hide', message=FALSE}
#Loading the packages
#ipak <- function(pkg){
#  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
#  if (length(new.pkg))
#    install.packages(new.pkg, dependencies = TRUE)
#  sapply(pkg, require, character.only = TRUE)
#}

#packages <- c('fBasics', 'fUnitRoots', 'forecast', 'nnet', 'zoo')
#ipak(packages)

library(fBasics)
library(fUnitRoots)
library(forecast)
library(nnet)
library(zoo)
```

```{r, echo=FALSE}
# test if there is at least one argument: if not, return an error
if (length(args)==0) {
  stop("At least one argument must be supplied (input file).\n", call.=FALSE)
} 

#data <-read.csv(params$args, header = TRUE)
data <- params$test
```

```{r, echo = F, warning = F, message = F}
message('data')
message(data[,2])
message(colnames(data))
```


```{r, echo = FALSE}
#first thing to check: is the data is constant ? if so, we cannot do much!
constant <- (length(unique(na.omit(data[,2]))) == 1)
#period set to 1, to avoid duplication of eval arguments in the chunks
if (constant){
  period <- 1
  pValue <- 0
  typ <- 'null'
}
```


```{r, echo=FALSE, eval = (constant == FALSE)}
#cuidado: lo primero que tengo que mirar es la longitud del periodo para poder crear la serie 
acf <- as.vector(acf(data[,2], plot = FALSE, na.action = na.pass )$acf)
```

```{r, echo = F, warning = F, message = F}
message('here')
message(constant)
message(acf)
```



```{r, echo = FALSE, eval = (constant == FALSE)}
findingLocalMax <- function (acf){
  localMax <- NULL
  for (i in 2:(length(acf) - 1)){
    previous <- acf[i -1]
    current <- acf[i]
    following <- acf[i + 1]
    
    if((previous < current) & (current > following)){
      localMax <- i
      break
    }
  }  
  return (localMax)
}

periodCorrelation <- findingLocalMax(acf)

period <- findfrequency(data[,2])
```


```{r, echo = F, warning = F, message = F}
message('period')
message(period)
```


```{r, echo=FALSE}
ts <- ts(data[,2], frequency = period)
name <- colnames(data)[2]
```



```{r, echo = F, warning = F, message = F}
message('period')
message(period)

message('ts')
message(ts)

message('length')
message(length(ts))
```

#Time series to evaluate: `r name`

### 1) Descriptive Analysis, Missing Values and Normalization

This time series represents the evolution of the number of `r name`. 

```{r, echo = FALSE}
plot(ts, ylab = name)
```


It contains $`r length(ts)`$ observations, going from `r as.POSIXct(data[1,1]/1000, origin="1970-01-01")` to `r as.POSIXct(data[length(data[,1]),1]/1000, origin="1970-01-01")`. 

```{r, echo = FALSE, eval = (constant == TRUE)}
const <- unique(na.omit(data[,2]))
```

`r if(constant == TRUE){paste('The values of the time series are constant, all of them are equal to ', unique(na.omit(data[,2])), '. Unfortunately, a constant series cannot be analyzed much more and so we end up the report here.', separator = '')}`

```{r, echo = FALSE, eval = (constant == FALSE)}
nas <- sum(is.na(ts))

if(nas == 0){
  chunki <- 'This time series does **not** contain any **missing values**, thus there is no need to make any interpolation.'
}

if (nas != 0){
  chunki <- paste('This time series contains ', nas, ' **missing values**. We will interpolate NAs linearly with the values that wrap the missing value. Missing values in the extremes of the series (first or last values) will be refilled with the closest point.')
  
  ts <- na.approx(ts, rule = 2)
}

min_ts <- min(ts)
max_ts <- max(ts)
mean_ts <- mean(ts)
```

`r if (constant == FALSE){paste('The time series contains values between ', min_ts, ' and ', max_ts, ' with a mean of ', round(mean_ts,3), '.', separator = '')}`

`r if (constant == FALSE){chunki}`

We will normalize the series in order to have all values between $0$ and $1$. This will be useful when comparing it with other time series that have different units. We display the plot of the normalized series.

```{r, echo = FALSE}
ts <- (ts - min_ts)/(max_ts - min_ts)
plot(ts, ylab = name)
```


`r if (constant == FALSE){'### 2) Computing the period length'}`

```{r, echo=FALSE, eval=(constant == FALSE)}
if(period == 1){
  chunk2 <- 'We have estimated the period length with the **spectral density** technique and the result leads us to think that there is not a clear period in our time series. Unfortunately, we can not provide much information when the series is non-periodic so we end up our report here.'
}

if (period !=1){
  chunk2 <- 'In order to tackle this problem of period length detection we use two different techniques: one with autocorrelation and one with
spectral density.

**Autocorrelation:**

We use the computation of the autocorrelation of the original series with a lagged version of itself to get a
possible period length. We would like to define as period length the lag index of the first local maximum that correpsonds to the highest correlation.

We display here the calculated coefficients:
'
  
}
pValue = 0
typ = 'null'
nullPresence <- FALSE
```

`r if(constant == FALSE) {chunk2}`

```{r, echo=FALSE, eval=(period!=1)}
plot(acf(ts, plot = FALSE), main = 'Autocorrelation with lagged versions')
```

```{r, echo=FALSE, eval=(period!=1 & findingLocalMax(acf) != NULL)}
chunk2bis <- paste('By looking for a local maximum, we obtain a period of length ', findingLocalMax(acf), sep = '')
```

```{r, echo=FALSE, eval=(period!=1 & findingLocalMax(acf) == NULL)}
chunk2bis <- paste('By looking at the plot we are not able to detect a local maximum so we cannot use this method.')
```

```{r, echo=FALSE, eval=(period!=1)}
  chunk2bisbis <- '
**Spectral Density:**
                     
Secondly, we try an approach that takes into account the spectral density. We know that the period is the
inverse of the frequency so we want to find the frequency that corresponds to the maximum of the spectral
density. 

We display the plot:'
```

`r if(period !=1) {chunk2bis}`

`r if(period !=1) {chunk2bisbis}`


```{r, echo=FALSE, eval=(period!=1)}
  plot(spec.ar(ts, plot = FALSE), main = paste('Spectral Density of', name))
```

`r if (period != 1) {paste('This method returns a period of length ', period, '. We have found, empirically, that in practice, this method performs better than the previous one. We will keep this result for the future.', sep = '')}`

`r if(period !=1) {'### 3) Checking stationarity'}`

```{r, echo=FALSE, eval=(period!=1), message=FALSE, warning=FALSE}

res <- adfTest(ts, lags = 0, type = "nc")
pValue <- res@test$p.value

testResult <- function (pValue){

  if (pValue < 0.05){
    return(paste('We use the **Dickey-Fuller** test in order to test the null hypothesis H0: ‘The series contains a unit root’ (i.e. is
not stationary). At a 0.05% level of confidence, the p-value (', round(pValue,3), ') is small and thus we have enough evidence to reject the null hypothesis and conclude that the series is indeed **stationary**. '));
  }

  else{
    return(paste('We use the **Dickey-Fuller** test in order to test the null hypothesis H0: ‘The series contains a unit root’ (i.e. is
not stationary). At a 0.05% level of confidence, the p-value (', round(pValue,3), ') is bigger enough to say that we fail to reject the null hypothesis and we conclude that the series is **not stationary**.'));
  }
}

differentiating <- function(pValue){
    if (pValue < 0.05){
      return ('The is no need to differentiate our series.')
    }
  else{
    return ('We want to check how many differentiations we need to perform in order to make it stationary.
We start lagging our series at level 1 and we increase the lagging parameter until the series is stationary.')
  }
}
```

`r if(period !=1) {testResult(pValue)}`

`r if(period !=1) {differentiating(pValue)}`


```{r, echo = F, warning = F, message = F}
message('ahora aqui')
message(ndiffs(ts))
```


```{r, echo=FALSE, eval=(period!=1 & pValue >= 0.05)}
lag <- ndiffs(ts)
if(lag == 0){lag = lag + 1}
plot(diff(ts, lag = lag), main = paste('Differentiation at lag = ', lag))
```

`r if(period !=1 & pValue >= 0.05) {paste('After ', lag, ' differenciations, we can say that the series is already stationary.')}`

`r if(period !=1) {'### 4) Decomposing the series'}`

```{r, echo=FALSE, eval=(period != 1), warning=FALSE}
#warning: null points are not allowed in the series as the estimation of the trend when using multiplicative type divides by 0. At first, we tried adding 0.00001 to the null values of the series. This idea distorted the result for the confidence intervals and had to be discarded. Finally, we decide to force the additive model on those cases.

nullPresence <- FALSE

if (0 %in% ts){
  acfAdd <- 0
  acfMult <- 0
  nullPresence <- TRUE
}
```

`r if(period !=1 & nullPresence == FALSE) {'In order to know the type of series we are dealing with, we make two decompositions: one with the additive assumption and one with the multiplicative one. This is done in order to check how much correlation between data points is still encoded within the residuals. We would like to choose the model with the lesser amount of autocorrelation in the residuals. This will mean that the chosen model encapsulates as much information as possible.'}`

`r if(period !=1 & nullPresence == TRUE){'The time series we are studying contains values that are equal to 0. This means that a multiplicative model is not possible. The reason behind this is because in a multiplicative model the trend is calculated as division of two data points and thus a zero value is forbidden. Instead the **additive model** uses the substraction of data points and so it is used in this case.'}`

`r if(period !=1 & nullPresence == FALSE) {'We take a look at the respective autocorrelations:'}`


```{r, echo=FALSE, eval=(period!=1 & nullPresence == FALSE)}
randomAdd <- na.approx(decompose(ts, type = "additive")$random, rule = 2)
randomMult <- na.approx(decompose(ts, type = "multiplicative")$random, rule = 2)

plot(acf(randomAdd, plot = FALSE), main ='Autocorrelation on the additive residuals')
plot(acf(randomMult, plot = FALSE), main = 'Autocorrelation on the multiplicative residuals')
```

```{r, echo=FALSE, eval=(period!=1 & nullPresence == FALSE)}
acfAdd <- sum(acf(randomAdd, plot = FALSE)$acf^2)
acfMult <- sum(acf(randomMult, plot = FALSE)$acf^2)
```

```{r, echo=FALSE, eval=(period!=1)}
addOrMult <- function(addCoeff, multCoeff){
  if (abs(addCoeff-multCoeff) <=  0.002){
    res = paste('In this case both auto-correlation coefficients are very **similar**. We obtained ', round(addCoeff,3), 'for the additive and ', round(multCoeff,3), 'in the multiplicative case. We are not in a position to choose one which is clearly better than the other one. We continue this report displaying solutions for both of the decompositions and we let the user choose which one suits the problem best.' )
  }
  
  else{
    if (addCoeff < multCoeff){
    res = paste('In this case the auto-correlation coefficient of the additive model (',round(addCoeff,3), ') is lesser than the multiplicative (',round(multCoeff,3),') so we conclude that fitting an **additive model** is more accurate.')
    }

    else{
    res = paste('In this case the auto-correlation coefficient of the multiplicative model (',round(multCoeff,3),') is lesser than      the additive (',round(addCoeff,3),') so we conclude that fitting a **multiplicative model** is more accurate.');
    }
  }
  return (res)
}

type <- function(addCoeff, multCoeff){
  if (abs(addCoeff-multCoeff) <=  0.002){
    return('both')
  }
  else{
    if (addCoeff < multCoeff){
      return('additive')
    }

    else{
      return ('multiplicative')  
    }
  }
}

typ <- type(acfAdd, acfMult)
```


```{r, echo=FALSE, eval=(period != 1 & nullPresence == TRUE)}
typ <- 'additive'
```



`r if(period !=1 & nullPresence == FALSE) {addOrMult(acfAdd, acfMult)}`

`r if(period !=1 & nullPresence == FALSE) {'We can now proceed to the decomposition of the series. We will split the series into trend, seasonal and random component.'}`

```{r, echo = F, warning = F, message = F}
message('aqui')
message('type')
message(typ)
```

```{r, echo=FALSE, eval=((period != 1) & (typ != 'both') & (typ != 'null'))}
plot(decompose(ts, type = typ))
```

```{r, echo = F, warning = F, message = F}
message('aqui2')
message('type')
message(typ)
```


```{r, echo=FALSE, eval=((period != 1) & (typ == 'both'))}
plot(decompose(ts, type = 'additive'))
plot(decompose(ts, type = 'multiplicative'))
```

`r if(period !=1) {'### 5) Detecting the order of the Auto-Regressive process'}`

`r if(period !=1) {'It is also interesting to check the **partial autocorrelations** plot in order to have better insights on the relationship between the observations. Briefly, the partial autocorrelation at lag k is the correlation that results after removing the effect of any correlations due to the terms at shorter lags. Partial Autocorrelation plots are a commonly used for identifying the order of an autoregressive process AR(p). We select as the order p the last lag that was statistically significant.'}`

```{r, echo=FALSE, eval=(period!=1)}
plot(pacf(ts, plot = FALSE), main = paste('Partial Autocorrelations of the', name, 'series'))
```

```{r, echo=FALSE, eval=(period!=1)}
#extracting the last significant lag
upper <- 1.96 / sqrt(length(ts))
lower <- -1.96 / sqrt(length(ts))

findOrder <- function(pacf){
  i <- 1
  while((pacf$acf[i] > upper ) | (pacf$acf[i] < lower)){
    i = i + 1
  }
  return(i - 1)
}

orderAR <- findOrder(pacf(ts, plot = FALSE))
```

`r if(period !=1) {paste('In the above plot we can see that after the lag number ', orderAR, ' the values are not longer significative. That\'s why we conclude that the underlying Auto Regressive process should be of order ', orderAR,'.', sep = '')}`

`r if(period !=1) {'### 6) Exponential Smoothing'}`

`r if(period !=1) {'In this section we focus on smoothing the series by applying the **Holt Winters** technique which consists on smoothing the three components of our series. This will allow us to predict some points ahead into the future and to filter anomalies (if any).'}`

`r if(period !=1 & typ != 'both') {paste('We choose the type of model we detected in section 4) which is ', type(acfAdd, acfMult),'. We make predictions for two periods into the future, this means predicting ', period * 2 , ' points. We will also provide two indicators of the goodness-of-fit of the model. The indicators alone do not provide much information but they are useful for a future model comparison/selection. We provide a brief description.', sep = '')}`

`r if(period !=1 & typ == 'both') {paste('We were not able to choose a model type in section 4) so we provide an exponential smoothing for both models. We make predictions for two periods into the future, this means predicting ', period * 2 , 'points. We will also provide two indicators of the goodness-of-fit of the models. The indicators alone do not provide much information but they are useful for a future model comparison/selection. We provide a brief description.')}`

`r if(period !=1) {'The **AIC** (Akaike Information Criterion) is an estimator of the relative quality of a model for a given set of data. Given a set of candidate models for the data, the preferred model is the one with the minimum AIC value. Thus, AIC rewards goodness of fit (as assessed by the likelihood function), but it also includes a penalty that is an increasing function of the number of estimated parameters. The penalty discourages overfitting, because increasing the number of parameters in the model almost always improves the goodness of the fit.'}`

`r if(period !=1) {'On the other hand, the **BIC** (Bayesian Information Criterion) indicator resemble the AIC (the lower the better) but with a penalty term larger than in AIC.'}`

```{r, echo = F, warning = F, message = F}
message('period')
message(period)

message('type')
message(typ)
```


```{r, echo=FALSE, eval=((period != 1) & (typ != 'both') & (typ != 'null')), warning=FALSE}
hw <- HoltWinters(ts, seasonal = typ)
p <-predict(hw, n.ahead = period * 2, prediction.interval = TRUE)
plot(hw, p, main = paste(typ, ' H-W smoothing'))
```

`r if((period !=1) & (typ != 'both')){paste('We have used the default optimization of the smoothing parameters alpha, beta and gamma which is the Nelder-Mead Optimization Method. We obtained as optimal values: alpha = ', round(hw$alpha, 4), ', beta = ', round(hw$beta, 3), ' and gamma = ' , round(hw$gamma, 3), '.', sep = '')}`

```{r, echo=FALSE, eval=((period!=1) & (typ != 'both') & (typ != 'null'))}
t <- 'ZZA'
if(typ == 'multiplicative'){
  t <- 'ZZM'
}

if (period > 24){
  #the ets function does not allow us to fit a TS model when the period is too long that is why we use the arima technique
  fit <- auto.arima(ts)
  aic <- fit$aic
  aicc <- fit$aicc
  bic <- fit$bic
}

if (period <= 24){
  fit<- ets(ts, model = t)
  aic <- fit$aic
  aicc <- fit$aicc
  bic <- fit$bic
}

```

`r if((period !=1) & (typ != 'both')){paste('In our case, the AIC is equal to ', round(aic,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`

```{r, echo=FALSE, eval=((period != 1) & (typ == 'both')), warning=FALSE}
hwA <- (HoltWinters(ts, seasonal = 'additive'))
pA <-predict(hwA, n.ahead = period * 2, prediction.interval = TRUE)
plot(hwA, pA, main ='Additive H-W smoothing')

fit<- ets(ts, model = 'ZZA')
aica <- fit$aic
aicc <- fit$aicc
bic <- fit$bic
```

`r if((period !=1) & (typ == 'both')){paste('We have used the default optimization of the smoothing parameters alpha, beta and gamma which is the Nelder-Mead Optimization Method. We obtained as optimal values: alpha = ', round(hwA$alpha, 4), ', beta = ', round(hwA$beta, 3), ' and gamma = ' , round(hwA$gamma, 3), '.', sep = '')}`

`r if((period !=1) & (typ == 'both')){paste('In the additive case, the AIC is equal to ', round(aica,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`

```{r, echo=FALSE, eval=((period != 1) & (typ == 'both')), warning=FALSE}
hwM <- (HoltWinters(ts, seasonal = 'multiplicative'))
pM <-predict(hwM, n.ahead = period * 2, prediction.interval = TRUE)
plot(hwM, pM, main = 'Multiplicative H-W smoothing')

fit<- ets(ts, model = 'ZZM')
aicm <- fit$aic
aicc <- fit$aicc
bic <- fit$bic
```

`r if((period !=1) & (typ == 'both')){paste('We have used the default optimization of the smoothing parameters alpha, beta and gamma which is the Nelder-Mead Optimization Method. We obtained as optimal values: alpha = ',round(hwM$alpha, 4), ', beta = ', round(hwM$beta, 3), ' and gamma = ' , round(hwM$gamma, 3), '.', sep = '')}`

`r if((period !=1) & (typ == 'both')){paste('In the multiplicative case, the AIC is equal to ', round(aicm,3),' and the BIC equals ', round(bic,3), '. These indicators can be useful in a future model selection.', sep = '')}`


`r if(period !=1) {'### 7) Filtering Anomalies'}`

`r if(period !=1) {'For this matter, we will use two different techniques: percentiles and confidence intervals.'}`

```{r, echo=FALSE, eval = (period !=1) & (typ == 'both')}
lastType <- function(aica, aicm){
  if (aica < aicm){
    return('additive')
  }
  else{
    return('multiplicative')
  }
}
```

`r if((period !=1) & (typ == 'both')) {paste('As we were not able to choose one type of model previously, we decide to perform the filtering on the model with the lowest AIC, which is ', lastType(aica, aicm), '.', sep = '')}`



`r if(period !=1) {'We will fit a Holt Winters smoothing as explained in the previous section, we will take a look at the errors produced (residuals = predicted value - actual value) and check their distribution.'}`

```{r, echo=FALSE, eval = ((period!=1) & (typ != 'both'))}
#we use the ets function here in order to have the same predictions length
#Warning: when the period is larger than 24 the ets function gives us an exception, we prefer to fit an arima model with the parameter D = 1 to take seasonal differenciating into account

if (period > 24){
  pred <- auto.arima(ts, D = 1)$fitted
}

if (period <= 24){
  pred <- ets(ts, model = t)$fitted
}

residuals<- ts - pred
hist(residuals, breaks = 50, main = 'Distribution of the residuals')
```

```{r, echo=FALSE, eval = ((period!=1) & (typ == 'both'))}
#we use the ets function here in order to have the same predictions length
t <- 'ZZA'
if (aica > aicm){
  t <- 'ZZM'
}
pred <- ets(ts, model = t)$fitted
residuals<- ts - pred
hist(residuals, breaks = 50, main = 'Distribution of the residuals')
```

`r if(period !=1) {'In the above plot we see the distribution of the residuals after the smoothing. There is no normality assumption in fitting an exponential smoothing nor in producing confidence intervals so we do not need a bell shaped histogram. We can proceed to the filtering.'}`

`r if(period !=1) {'**Using percentiles: **'}`


`r if(period !=1) {'We will use percentiles to determine which errors are too big to be considered normal. For now, we will take the 95 percentile but this can be set as a parameter in future versions.'}`

`r if(period !=1) {'We will take as anomalies the points that produced positive errors that are beyond the 97.5 percentile and negative errors that are lower than the 2.25 percentile.'}`

`r if(period !=1) {'We can now filter the data, the detected anomalies are the blue points.'}`


```{r, echo=FALSE, eval=(period!=1)}

upper<- quantile(residuals, 0.95)
lower<- quantile(residuals, 0.05)

plot(as.vector(ts), main = 'Filtering with percentiles', type = 'l', xlab = 'Time', ylab = 'Series')
lines(1:length(ts), pred, col = 'red')
points(which((residuals < lower) | (residuals > upper)), ts[(residuals < lower) | (residuals > upper)], pch = 19, col = 'blue')
```

`r if(period !=1) {'**Using confidence intervals: **'}`

`r if(period !=1) {'We use here a more "classic" technique. The errors are normally distributed so we use gaussian confidence intervals formulas with a confidence level equal to 0.05 (shaded area). The blue points are the detected anomalies.'}`

```{r, echo=FALSE, eval=(period!=1)}
upper<- as.vector(pred + sqrt(var(residuals)) *2)
lower<- as.vector(pred - sqrt(var(residuals)) *2)

plot(1:length(ts), lower, type = 'n', main = 'Filtering with confidence intervals', xlab = 'Time', ylab = 'Series', ylim = c(min(min(lower), min(ts)), max(max(upper), max(ts))))
lines(1:length(ts), upper, col = 'grey')
lines(1:length(ts), lower, col = 'grey')
polygon(c(1:length(ts), rev(1:length(ts))), c(upper, rev(lower)),
     col = "grey80", border = NA)

lines(1:length(ts), ts, col = 'black')
lines(1:length(ts), pred, col = 'red')

points(which((ts < lower) | (ts> upper)), ts[(ts < lower) | (ts > upper)], pch = 19, col = 'blue')
```

